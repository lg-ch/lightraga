import feedparser
import requests
import os
import logging
from datetime import datetime


    title = entry.get("title", "no_title")
    link = entry.get("link")


    resp = requests.get(link, headers=headers)
    resp.raise_for_status()
    html = resp.text

    fname = sanitize_filename(title)[:80]
    date_part = datetime.now().strftime("%Y-%m-%d")
    save_path = os.path.join(OUTPUT_DIR, date_part)
    os.makedirs(save_path, exist_ok=True)

    html_path = os.path.join(save_path, fname + ".html")
    pdf_path = os.path.join(save_path, fname + ".pdf")




def main():
    last_id = load_last_processed()
    feed = feedparser.parse(RSS_URL)
    entries = feed.entries
    # tri par date ascendante
    entries = sorted(entries, key=lambda e: e.published_parsed)
    for entry in entries:
        entry_id = entry.get("id") or entry.get("link")
        if last_id and entry_id <= last_id:
            continue
        # nouveau
        download_article(entry)
        # mise à jour du dernier id traité
        save_last_processed(entry_id)
    logging.info("Done.")
